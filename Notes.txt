Ref : 
DP 900 : https://gist.github.com/j-thepac/61551f37b589bd425159c19064393987
ETL : https://gist.github.com/j-thepac/dc7e292b16a82c4f0150091477017987
Synapse: https://gist.github.com/j-thepac/a06be89cffb5cfd08bf63646487b186b
Spark Notes: https://docs.google.com/document/d/1J8NaG9RLkVxrOc_4pxjEtxjN7KAPx6SbUSrKkwOL6Ks/edit?usp=drive_link
--------------
- Parq vs Avro
  - Parq: 
    - Column Based 
    - Easy to Read
    - Schema is maintained
  - Avro
    - Row Based
    - Easy to Write
-------------------
- storage accounts
  - https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview
  - https://learn.microsoft.com/en-us/training/modules/secure-azure-storage-account
  - https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control
  - Advanced Threat Protection
    - Microsoft Defender for Cloud.
    - Detects unusual and potentially harmful attempts to access or exploit storage accounts.
  - Larger files lead to better performance and reduced costs.
  - Partitioning
    - Batch
      {Region}/{SubjectMatter(s)}/In/{yyyy}/{mm}/{dd}/{hh}/
      {Region}/{SubjectMatter(s)}/Out/{yyyy}/{mm}/{dd}/{hh}/
      {Region}/{SubjectMatter(s)}/Bad/{yyyy}/{mm}/{dd}/{hh}/
    Eg:USA/Extracts/ACMEPaperCo/In/2017/08/14/updates_08142017.csv
    - TimeSeries
      /DataSet/YYYY/MM/DD/datafile_YYYY_MM_DD.tsv
  - Medaillion Architecture : Bronze (Raw) , Silver (EZ), Gold (CZ)
  - Lake house = Medallion + Delta Lake  + ADSL 
  - Access 
      RBAC vs POSIX vs ACL(access control list ) vs SAS vs Network 
      - Network
        - Range of IP addr (Storage > Networking > )

      - RBAC (Role-Based Access Control)
        - based on roles

      - POSIX (Portable Operating System Interface)
        - chmod -R 755 my_directory

      - ACL (access control list)
        - fine-grained access control 
        - 421 : RWX (For Delete every directory within it, requires Read + Write + Execute )

      - SAS (Shared Access Signatures)
        - token to grantaccess attached to a URI.
        - Storage > Security > Access keys > Primary and Secondary
        - Types
          - Service Level  -allow access to specific resources 
          - Account LEvel - Service level Access + More eg: create file systems.

      - Blob Storage	https://*mystorageaccount*.blob.core.windows.net/*mycontainer*/*myblob*
        Static website (Blob Storage)	https://<storage-account>.web.core.windows.net
        Data Lake Storage	https://<storage-account>.dfs.core.windows.net
        Azure Files	https://<storage-account>.file.core.windows.net
        Queue Storage	https://<storage-account>.queue.core.windows.net
        Table Storage	https://<storage-account>.table.core.windows.net
      - Upgrade and Movement to another region is possible 

    - Gen2 
      - Data residency affect performance
      - Geo-replication will affect pricing
      - https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices
      - Data Residence : Large File less Cost and more Performance
      
    - Blob 
      - (https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#structure-data-sets 
      -  https://learn.microsoft.com/en-us/training/modules/store-app-data-with-azure-blob-storage/3-design-organization-strategy )
      -https://learn.microsoft.com/en-us/training/modules/secure-azure-storage-account/
      - flat
      - Folder Structure -  domain/subDomain/2017/q1.xls
      - 3 types : Block (independent, upload , Download) , APpend (Stream ,Logs) , Page (random access used by VM)
      - Reading of files is billed with a 4-MB operation
      -  automatically encrypted by Storage Service Encryption (SSE) with a 256-bit Advanced Encryption Standard (AES) cipher
      - Support CORS (cross-origin resource sharing ) Api
      - Access
        - Storage Blob Data Owner: All access 
        - Storage Blob Data Contributor:  allows for read, write, and delete 
        - Storage Blob Data Reader allows for read access
        - Storage Blog Data Delegator : user delegation key ,used to sign SAS tokens.
-------------------
- SQL / Dedicated /  Azure Synapse Analytics database 
  - https://learn.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning
  - https://learn.microsoft.com/en-us/training/modules/choose-storage-approach-in-azure/
  - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/massively-parallel-processing-mpp-architecture 
  - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/quickstart-scale-compute-tsql#check-dedicated-sql-pool-formerly-sql-dw-state
  - https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/
  - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-waiting-queries
  - Scale out = better performance, or scale back compute to save costs.
  - Sys Objects
    - SELECT name FROM sys.objects WHERE type = 'V' AND name LIKE 'dm_pdw%';
    - EXEC sp_help 'sys.objects';
  - Queries
    - COPY INTO [dbo].[T] FROM 'https://BlobStorage.blob.core.windows.net/Folder'
      WITH ( FILE_TYPE = 'CSV',
            FIELDTERMINATOR = ',',
            FIELDQUOTE = '')
    - CREATE TABLE dbo.T2
        WITH
        ( DISTRIBUTION = REPLICATE,
          CLUSTERED COLUMNSTORE INDEX )
        AS Select * from T 
    - INSERT INTO dbo.T Select * from dbo.T2
    - CETAS
        CREATE EXTERNAL FILE FORMAT [ParquetFF] WITH (
            FORMAT_TYPE = PARQUET,
            DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
        );
        GO
        CREATE EXTERNAL DATA SOURCE [SynapseSQLwriteable] WITH (
            LOCATION = 'https**://<mystoageaccount>.dfs.core.windows.net/<mycontainer>/<mybaseoutputfolderpath>',
            CREDENTIAL = [WorkspaceIdentity]
        );
        GO
        CREATE EXTERNAL TABLE [dbo].[<myexternaltable>] WITH (
                LOCATION = '<myoutputsubfolder>/',
                DATA_SOURCE = [SynapseSQLwriteable],
                FILE_FORMAT = [ParquetFF]
        ) AS
        SELECT * FROM [<myview>];
  - BucketBy vs partitionBy 
    - Bucket Same values in 1 Bucket
    - df.write.partitionBy("year").bucketBy(2, "id").mode("overwrite").saveAsTable("bucketed_table")
  - Partitioning
    - https://learn.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning#designing-partitions
    - Sharding  
        - horizontal partitions (each record into different Partition) 
        - Same Schema
        - https://learn.microsoft.com/en-us/azure/architecture/patterns/sharding
        - WITH ( PARTITION KEY = C);
        - Types
          - LookUp : 
              - Requires look Up table
              -  shard key value maps to a physical partition
          - Hash partitioning : Use hash function ,even data and load distribution.
          - Range partitioning : 1 to n in shard A ,  n+1 to m  stored in shard B

    - Vertical Paritioning 
        - Each Column of Data in Seperate Partition /Shard eg: Avro
        - When Only particular columns are important 

    - Functional Paritioning
        - Logic based 

  - Cluster vs CLuster ColumnStore
  - Distribution
    - Hash = querying Easier
    - Round robin =loading data is simple
    - Replicated = small tables.

  - Change data capture VS change tracking Vs merge replication Vs snapshot replication
    - Change Tracking: 
      - Uses a internal log 
      - lightweight 
      -  Tracks row was changed without tracking the data that was changed

    - Change Data Capture (CDC):
        - Internally uses a seperate Table (sys.sp_cdc_enable_table)
        - Identifies Each Row That Has Changed
        - Better than Change tracking

    - Merge Replication: 
      - complex solution 
      - Merge

    - Snapshot Replication: 
      - snapshot of the data at a specific point in time 
      -  does not track changes incrementally.
      
  - CREATE MATERIALIZED VIEW mv WITH (distribution = hash(c), FOR_APPEND) AS SELECT * FROM T  ;
    - Data with snapshot is created 
    - REFRESH MATERIALIZED VIEW employee_summary; -- Data is static and needs to be refreshed
-------
## HTAP (Hybrid Transactional and Analytical Processing Patterns)
- Data Sync bw Src (SQL server , Azure SQL )and Target -Dataverse(Datalakes) ,Dedicated and Cosmos targets
-  Src > Synapse > Target (Synapse as Compute Engine)
- Cosmos DB (No SQL) 
	- Select Type of Cosmos DB (SQL ,Mongo etc., ) while setting up
	- Fast Read and Write
	- Auto Scales
	- Uses API
	- Expensive
	- To Enable HTAP (On Demand - video8): 
		- Cosmose Homepage : To turn on Analytical Contianer in Cosmos
		- In Synapse: Homepage > Integration > Enable Synapse link
	- Synapse read cosmos: Select * from openrowset() with()as T CROSS APPLY OPENJSON(C) with()
	
- SQL server
	-  Use managed Identnity
	-  Azure Synapse Link enabled 
	-  Add ips to Firewall 
	-  Create linked Service + Linked CONNECTION in Synapse(Piplnes > "+")
	-  Consfiure Blob Storage for intermiitend result
-------------------
- Stream Analytics (OnDemand - video 9)
  - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-solution-patterns
  - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization
  - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions
  - Create Stream Analytics in Azure Portal
    - Stream Analyics > HomePage (No Seperate GUI)
    - Create input (Event Hub/IoT Hub/ Blob /kafka)
    - Create Query 
    - Create output  (All types supported) 
    - Monitor 
  - Error: "was sent later than configured tolerance" - Chnage withWaterMark(“15 minutes”) 
  - 6 SUs = 1 node
  - parallelization = partition key
  - Each Statement (select )= 1 SU (Streaming Unit ) and Each partition = 1 SU 
  - Streaming : When the Backlogged Input Events metric > zero, the job is not able to process all incoming events.Increase SU (Streaming Units)
  - Temporal Window (Time Based):
    - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions
    - Types :
        - Tumbling : Aggregate all events for that same time interval. 
			-- The number of cars within a three-minute window going through a toll station that has three tollbooths":
		        SELECT COUNT(*) AS Count, TollBoothId into stream_analytics 
			INTO [output] 
		        FROM iot_event_hub 
			TIMESTAMP By TS_Kafka  -- 
		        PARTITION BY PartitionId  --  (similar to window partition)
		        GROUP BY  tumbling_window(second,2), TollBoothId, PartitionId
		        ``
        - Hopping : Tumbling + Overlapping for standard time with previous Window
		GROUP BY HoppingWindow(second, 60, 30)
        - Sliding : Window is created by an event and window exists for set time 
		GROUP BY slidingwindow(minute,1)
        - Session : 
		Window Cannot Overlap 
		Stops when stream stops and Resume if stream resumes 
		Window can have 1 or more event bit not reverse 
		Avoid Nulls by capturing Empty Windw 
		GROUP BY SessionWindow(second, 20, 60) 
        - Snapshot : Select count(*) into T from T1 group by system.timestamp()
		GROUP BY kafka_TS

  - Streaming : When the Backlogged Input Events metric > zero, the job is not able to process all incoming events.Increase SU (Streaming Units) 
-------------------
MS PurView
- Governance
- Avoid Data Duplication 
- Consists of Data Owner (Policy Creators), Stewards(enforce policy) , publishers , Consumers(Data Engineers)  
- Store Information on type of Data which exists , Record Chnages of Data LandScape in a Company
------------
- Lambda vs kappa
  - Lambda : Raw and Streaming served by different activities in same pipeline
  - Kappa : Both Raw and Streaming is done by same activity in same pipleien (Trigger =once , continues /processingtime))
-------------------
- Azure HDInsight
  - Apache Spark
  - Apache Hive
  - LLAP
  - Apache Kafka
  - Hadoop and more, in your Azure environment.
-------------------
- Databricks 
  - Simialr to Synapse Notebooks 
  - SELECT /*+ SKEW('orders', 'o_custId') */ * FROM orders, customers WHERE o_custId = c_custId; --Skew Data
  -  Types:
    - Standard Cluster: General-purpose, suitable for various workloads.
    - High Concurrency Cluster: Optimized for concurrent users and low-latency queries.
    - Job Cluster: Designed for scheduled jobs, cost-effective with auto-termination.
    - Interactive Cluster: For ad-hoc analysis and interactive workloads.
    - Single Node Cluster: For development and testing with a single node.
    - Autoscaling Cluster: Automatically adjusts resources based on demand.
-------------------
- Difference Bwtween : https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing
  - Azure Stream Analytics
  - HDInsight with Spark Streaming
  - Apache Spark in Azure Databricks
  - Apache Kafka streams API
  - Azure Functions
  - Azure App Service WebJobs

-------------------
- ADF:
  - https://learn.microsoft.com/en-us/azure/data-factory/monitor-configure-diagnostics  
  - CLI cmd to tart ADF  $Invoke-AzDataFactoryV2Pipeline
  - https://learn.microsoft.com/en-us/azure/data-factory/pipeline-trigger-troubleshoot-guide
  -  Settings > Diagnostci Setting > Send to Log Analytics>Various logs relevant to your workloads to send to Log Analytics tables. 
-------------------
- Microsoft Purview (LeanIX ) 
  - https://learn.microsoft.com/en-us/purview/concept-data-lineage
  - https://learn.microsoft.com/en-us/purview/unified-catalog-search
  - Catalog and Governance and Audit
  - data lineage
  - Create PurView > Add DataSource > Scan using Managed Identity > Analyse Result using PBI or Builtin
  - master key in the Azure SQL database for lineage to work.
  - Microsoft Purview governance portal : view and search Metadata
-------------------
- Hive :
  - https://learn.microsoft.com/en-us/azure/data-factory/tutorial-transform-data-hive-virtual-network
  - HDInsight cluster URL :  https://<clustername>.azurehdinsight.net.
  - CREATE EXTERNAL TABLE T (id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' 
    STORED AS TEXTFILE LOCATION 'wasb://data@datastg.blob.core.windows.net/devices/';
    INSERT OVERWRITE TABLE T
    Select id FROM T2
-------------------
- Synapse :
  - https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/2-understand-network-security-options
  - Lookup : Denormalize
  - security options
    - Firewall rules : Type of IP that is allowed  Eg : Synapse > Security > Firewall > Add IPS
    - Virtual networks (VPN) : 
      - to securely communicate with Other Networks 
      - Managed workspace Virtual Network : 
        - managed By Synapse (spark Cluster , Dedicated, Serverless  )
        - Check box while creating Synapse 
    - Private endpoints:to connect up its various components through private IP address
    - Cosmos + Spark + LinkedService
      df = spark.read #write
          .format(""cosmos.olap"")\
          .option(""spark.synapse.linkedService"", ""my_linked_service"")\
          .option(""spark.cosmos.container"", ""my-container"")\
          .load()
        %%sql
        CREATE TABLE products using cosmos.olap options (
          spark.synapse.linkedService 'my_linked_service',
          spark.cosmos.container 'my-container'
        ); -- **SELECT productID, productName FROM products;"
    - Serverless + Cosmos + LS
      	SELECT * FROM OPENROWSET('CosmosDB',
        'Account=my-cosmos-db;Database=my-db;Key=abcd1234....==',
        [my-container]) as T
  - saveAsTable vs saveAsTempTable
---------------
## Glossary
### General
- Data integration:--	  Establishing links between data
- Data consolidation:--	To extract Data from multiple data sources into a consistent structure
- Data Lake Storage Security:--   Access control lists (ACLs) and Portable Operating System Interface (POSIX) permissions -you can set permissions at a directory level or file level
- Data Lake redundancy:--	provide data redundancy in a single data center with locally redundant storage (LRS), or to a secondary region by using the Geo-redundant storage (GRS) option.
- Data Lake Hierarchical Namespace:--	To enable  you can select the option to Enable hierarchical namespace while creating Storage account
- Blob vs GEn2:--	blob = flat namespace , Gen2 = directories
- Synapse:--	https:--//microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/01-Explore-Azure-Synapse.html
- Cosmos DB and Azure SQL:--	Can be used an Operational / Transaction DB as they support data Auto sync with Synapse (analytical store support)
- **Database template**:	Templetes offered in Synapse to create out of the box databasese like - **Agriculture , Automotive etc.,

### Security
- Microsoft Entra ID:--	"directory service contain secured objects - user accounts , Resource Accounts 
 If a user and an instance of Azure Synapse Analytics are part of the same Microsoft Entra ID ,then user can access synapse"
- Azure Active Directory (Entra ID):--	Used to Create 1 Functional ID (secret ) called Serive Principle and used across all applications to communicate with each other
- Managed identity (service principal):--	managed by Azure
- SP (Serivce Principle):--	Managed by User ie., renew etc.,
- FireWalls:--	IP address that is allowed or denied access to an Azure Synapse workspace
- Azure Virtual Network (VNet):--	Azure resources to securely communicate with other virtual networks,
- Managed workspace Virtual Network:--	The Virtual Network managed by Azure Synapse
- Private endpoints:--	"enables you to securely connect up its various components through endpoints
 uses a private IP address from your Virtual Network"
- Managed private endpoints:--	Private endpoints in Managed workspace Virtual Network.
- Conditional Access:--	if a user wants to access a resource, then they must complete Like Authentication
- SQL Authentication:--	Usn /Pass
- SAS:--	A security token that can be attached to a URI (bearer)
- Transparent data encryption:-- Encrypting data at rest (Dedicated and Blob) ie., Complete Container is encrypted


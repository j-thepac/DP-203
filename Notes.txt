Ref : 
DP 900 : https://gist.github.com/j-thepac/61551f37b589bd425159c19064393987
ETL : https://gist.github.com/j-thepac/dc7e292b16a82c4f0150091477017987
Synapse: https://gist.github.com/j-thepac/a06be89cffb5cfd08bf63646487b186b
Spark Notes: https://docs.google.com/document/d/1J8NaG9RLkVxrOc_4pxjEtxjN7KAPx6SbUSrKkwOL6Ks/edit?usp=drive_link
--------------
- Parq vs Avro
  - Parq: 
    - Column Based 
    - Easy to Read
    - Schema is maintained
  - Avro
    - Row Based
    - Easy to Write
-------------------
- storage accounts \ PolyBase
  - Each storage account located in different geographies 
  - Container = Logical Seperation
  - Types = Blob , gen2	
  - Also called as OnDemand , Datalake
  - Not a relationla Data Store
  - Able to Query using Serverless
  - Used for unplanned or "bursty" workloads , Data exploration , Data transformation , Data Warehouse
  - Larger files lead to better performance and reduced costs.
  - locally redundant storage (LRS),/ Geo-redundant storage (GRS) option
  - Hadoop Distributed File System (HDFS) 
  - Access control lists (ACLs) and Portable Operating System Interface (POSIX) 
  - Advanced Threat Protection using Microsoft Defender for Cloud (Detects unusual and potentially harmful attempts to access or exploit storage accounts)
	
  - Partitioning
    - Batch
      {Region}/{SubjectMatter(s)}/In/{yyyy}/{mm}/{dd}/{hh}/
      {Region}/{SubjectMatter(s)}/Out/{yyyy}/{mm}/{dd}/{hh}/
      {Region}/{SubjectMatter(s)}/Bad/{yyyy}/{mm}/{dd}/{hh}/
    Eg:USA/Extracts/ACMEPaperCo/In/2017/08/14/updates_08142017.csv
    - TimeSeries
      /DataSet/YYYY/MM/DD/datafile_YYYY_MM_DD.tsv
  - Medaillion Architecture : Bronze (Raw) , Silver (EZ), Gold (CZ)
  - Lake house = Medallion + Delta Lake  + ADSL 

  - Access : 
      - Service Principle : Functional ID created in Azure Active Directory
      - Managed Identity : Functional ID managed by Azure 
      - Firewall (Network) : Range of IP addr (Storage > Networking > )
      - RBAC (Role-Based Access Control): based on roles
      - ACL (access control list): Each file and directory in your storage account  (chmod)
      - SAS (Shared Access Signatures)
        - token to grantaccess attached to a URI.
        - Storage > Security > Access keys > Primary and Secondary
        - Types
          - Service Level  -allow access to specific resources 
          - Account LEvel - Service level Access + More eg: create file systems.

  - Endpoints for different Storages : 
      - Blob Storage	https://*mystorageaccount*.blob.core.windows.net/*mycontainer*/*myblob*
        Static website (Blob Storage)	https://<storage-account>.web.core.windows.net
        Data Lake Storage	https://<storage-account>.dfs.core.windows.net
        Azure Files	https://<storage-account>.file.core.windows.net
        Queue Storage	https://<storage-account>.queue.core.windows.net
        Table Storage	https://<storage-account>.table.core.windows.net
      - Upgrade and Movement to another region is possible 

    - Blob 
      - (https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#structure-data-sets 
      -  https://learn.microsoft.com/en-us/training/modules/store-app-data-with-azure-blob-storage/3-design-organization-strategy )
      -https://learn.microsoft.com/en-us/training/modules/secure-azure-storage-account/
      - flat
      - Folder Structure -  domain/subDomain/2017/q1.xls
      - 3 types : Block (independent, upload , Download) , APpend (Stream ,Logs) , Page (random access used by VM)
      - Reading of files is billed with a 4-MB operation
      -  automatically encrypted by Storage Service Encryption (SSE) with a 256-bit Advanced Encryption Standard (AES) cipher
      - Support CORS (cross-origin resource sharing ) Api
      - Access
        - Storage Blob Data Owner: All access 
        - Storage Blob Data Contributor:  allows for read, write, and delete 
        - Storage Blob Data Reader allows for read access
        - Storage Blog Data Delegator : user delegation key ,used to sign SAS tokens.

    - Gen2 
	- Blob + Hierarchical Namespace option to Enabled
	- Data residency affect performance
	- Geo-replication will affect pricing
	- https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices
	- Data Residence : Large File less Cost and more Performance
	- BucketBy vs partitionBy 
		- Partitions =parallel processing,  buckets = optimize join operations. 
		- BucketBy - Fixed groups of data based on a hash of specified columns
		- df.write.partitionBy("year").bucketBy(2, "id").mode("overwrite").saveAsTable("bucketed_table")

    - Query:
      - CSV
		SELECT * FROM OPENROWSET
			(BULK 'https://<>.blob.core.windows.net/*.csv',FORMAT = 'csv',PARSER_VERSION = '2.0',HEADER_ROW = TRUE) AS rows
			WITH (product_id INT) AS rows
      - JSON
		SELECT * FROM OPENROWSET
			(BULK 'https://mydatalake.blob.core.windows.net/*.json',FORMAT = 'csv'
				,FIELDTERMINATOR ='0x0b',FIELDQUOTE = '0x0b',ROWTERMINATOR = '0x0b') 
			WITH (c NVARCHAR(MAX)) as rows
      - Parquet
		SELECT * FROM 
			OPENROWSET(BULK 'https://<>.blob.core.windows.net/data/orders/year=*/month=*/*.*',FORMAT = 'parquet') AS orders

      - CETAS ( Cannot alter External Table)
	        CREATE EXTERNAL FILE FORMAT [ParquetFF] WITH (
	            FORMAT_TYPE = PARQUET,
	            DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
	        );
	        GO;
	        CREATE EXTERNAL DATA SOURCE [SynapseSQLwriteable] WITH (
	            LOCATION = 'https**://<mystoageaccount>.dfs.core.windows.net/<mycontainer>/<mybaseoutputfolderpath>',
	            CREDENTIAL = [WorkspaceIdentity]
	        );
	        GO;
	        CREATE EXTERNAL TABLE [dbo].[<myexternaltable>] WITH (
	                LOCATION = '<myoutputsubfolder>/',
	                DATA_SOURCE = [SynapseSQLwriteable],
	                FILE_FORMAT = [ParquetFF]
	        ) AS
	        SELECT * FROM [<myview>];

-------------------
- Dedicated /  Azure Synapse Analytics database / SQL Pool
	
  - Scale :Scale out = better performance, or scale back compute to save costs.
  - DW ( Node =computer , Data is divided into parition in each node for multithreading) 
	 - Node1 (Distribution1 = Partition1 ,Parition2...) 
	 - Node2  (Distribution2 = Partition1 , Partition2...) .... 
	 - 
	 - Node 60 (....
  - Monitor Performance
	OPTION :SELECT * FROM a , b where a.id = b.id
		OPTION (Label = 'CustJoin', HASH JOIN) #Instruct to Use Hash join  
	sys.dm_exec_sessions : all active user connections 
	sys.dm_pdw_exec_requests: all requests that are currently
	sys.dm_pdw_request_steps: about each step of a SQL request
	sys.dm_pdw_sql_requests: SQL requests that have been submitted
	sys.dm_pdw_dms_workers: the workers responsible for data movement
	sys.dm_pdw_waits:amount of time that requests have spent waiting for resources.
	DBCC PDW_SHOWEXECUTIONPLAN: Query Plan and the estimated costs

  - Distribution(for parallel , col =Skey,naturalKey), Index(Searching) , Partition (part of distribution , col=Range Col )
  - Sys Objects
    - SELECT name FROM sys.objects WHERE type = 'V' AND name LIKE 'dm_pdw%';
    - EXEC sp_help 'sys.objects';

  - Queries
    - COPY INTO [dbo].[T] FROM 'https://BlobStorage.blob.core.windows.net/Folder'
      WITH ( FILE_TYPE = 'CSV',
            FIELDTERMINATOR = ',',
            FIELDQUOTE = '')
    - CREATE TABLE dbo.T2
        WITH
        ( DISTRIBUTION = REPLICATE
          ,CLUSTERED COLUMNSTORE INDEX
	  ,Partition =C)
        AS Select * from T 

    - INSERT INTO dbo.T Select * from dbo.T2

  - Partitioning (stored within a distribution ,  )
    - Use a range column ( Timestamp )
    - https://learn.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning#designing-partitions
    - horizontal partitioning, you can improve the performance of the data load
    -  vertical partitioning, different parts of the database can be isolated from each other to improve cache use.
    - Sharding  
        - horizontal partitions (each record into different Partition) 
        - Same Schema
        - https://learn.microsoft.com/en-us/azure/architecture/patterns/sharding
        - WITH ( PARTITION KEY = C);
        - Types
          - LookUp : 
              - Requires look Up table
              -  shard key value maps to a physical partition
          - Hash partitioning : Use hash function ,even data and load distribution.
          - Range partitioning : 1 to n in shard A ,  n+1 to m  stored in shard B

    - Vertical Paritioning 
        - Each Column of Data in Seperate Partition /Shard eg: Avro
        - When Only particular columns are important 

    - Functional Paritioning
        - Logic based 
  - Partition switching 
	- Remove or replace a section of a table with dummy
	Steps: 
	- CREATE TABLE dbo.FactInternetSales_20000101 WITH    (  same as main table )
	- ALTER TABLE FactInternetSales SWITCH PARTITION 2 
		TO 
		FactInternetSales_20000101 PARTITION 2;
	
  - Cluster vs CLuster ColumnStore vs heap(no Index)
  -  60 Million = minimum  number of rows for clustered columnstore indexes  
  - Distribution
    - Hash = querying Easier , Large Tables and Good perofrmance for Joins 
    - Round robin =loading data is simple ,  small Tables and bad performance for Joins 
    - Replicated = small tables.
      
  - CREATE MATERIALIZED VIEW mv WITH (distribution = hash(c), FOR_APPEND) AS SELECT * FROM T  ;
    - Data with snapshot is created 
    - If query result is saved , data can be queird faster
    - REFRESH MATERIALIZED VIEW employee_summary; -- Data is static and needs to be refreshed

  - Masking
	- SQL home page > Dynamic Data Masking blade > Add MASk tab
	
  - transparent data encryption (TDE)
	- TDE encrypts the entire database storage using a symmetric key called a Database Encryption Key (DEK)
	- encryption algorithm AES256.
	-  DEK will then be used for decryption and re-encryption of the database
	- Keys :
		- service-managed certificated, (service-managed transparent data encryption)
		- asymmetric key that is stored in Azure Key Vault (customer-managed transparent data encryption)/Bring Your Own Key (BYOK) 
	-  TDE protector is set on the server level. 
--------------------
- Synapse :
  - https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/2-understand-network-security-options
  - Lookup : Denormalize
  - Compute 
	Linked Service : Service to Connect to different DataSources (External And Internal)
	Integrated Runtime : Compute which executes the DataFlow
	Source(Linked Service) > Operation > Sink(Linked Service)
  - security options
    - Firewall rules : Type of IP that is allowed  Eg : Synapse > Security > Firewall > Add IPS
    - Virtual networks (VPN) : 
      - to securely communicate with Other Networks 
      - Managed workspace Virtual Network : 
        - managed By Synapse (spark Cluster , Dedicated, Serverless  )
        - Check box while creating Synapse 
    - Private endpoints:to connect up its various components through private IP address
    - Cosmos + Spark + LinkedService
      df = spark.read #write
          .format(""cosmos.olap"")\
          .option(""spark.synapse.linkedService"", ""my_linked_service"")\
          .option(""spark.cosmos.container"", ""my-container"")\
          .load()
        %%sql
        CREATE TABLE products using cosmos.olap options (
          spark.synapse.linkedService 'my_linked_service',
          spark.cosmos.container 'my-container'
        ); -- **SELECT productID, productName FROM products;"
    - Serverless + Cosmos + LS
      	SELECT * FROM OPENROWSET('CosmosDB',
        'Account=my-cosmos-db;Database=my-db;Key=abcd1234....==',
        [my-container]) as T
  - saveAsTable vs saveAsTempTable
  - A logic app can be triggered by an email, and then run a pipeline.
-------
## HTAP (Hybrid Transactional and Analytical Processing Patterns)
	- Data Sync bw Src and Target
		src : SQL server , Azure SQL 
		Target: Dataverse(Datalakes) ,Dedicated and Cosmos targets
	-  Flow: Src > Synapse > Target (Synapse as Compute Engine)
	- not supported for Azure SQL Managed Instance.
	- Cosmos DB (No SQL) 
		- Select Type of Cosmos DB (SQL ,Mongo etc., ) while setting up
		- Fast Read and Write
		- Auto Scales
		- Uses API
		- Expensive
		- To Enable HTAP (On Demand - video8): 
			- Cosmose Homepage : To turn on Analytical Contianer in Cosmos
			- In Synapse: Homepage > Integration > Enable Synapse link
		- Synapse read cosmos: Select * from openrowset() with()as T CROSS APPLY OPENJSON(C) with()
		- When data type of column is changed - new column is created 	
		- Process :
			- Navigation Cosmos homepage > Synapse link blade > Enable
			- Open Synapse > Create linked Service for Cosmos
	- Azure SQL Database (Paas)
		- Target = dedicated SQL pool
		-  Use managed Identity
		- data types not supported ,Binary, nVarchar, varchar(Max)
		-  change feed feature in Azure SQL Database ie., data modification in memory and forwards to Synapse .
		-  Azure Synapse Link enabled 
		-  Add ips to Firewall 
		-  Create linked Service + Linked CONNECTION in Synapse(Piplnes > "+") ,then configure the mappings between the source and target tables.
		-  Configure Blob Storage for intermiitend result
-------------------
- Stream Analytics (OnDemand - video 9)
  - Create Stream Analytics in Azure Portal
    - Stream Analyics > HomePage (No Seperate GUI)
    - Create input (Event Hub/IoT Hub/ Blob /kafka)
    - Create Query 
    - Create output  (All types supported) 
    - Monitor 
  - Error: "was sent later than configured tolerance" - Chnage withWaterMark(“15 minutes”) 
  - 6 SUs = 1 node
  - parallelization = partition key
  - Each Statement (select )= 1 SU (Streaming Unit ) and Each partition = 1 SU 
  - Streaming : When the Backlogged Input Events metric > zero, the job is not able to process all incoming events.Increase SU (Streaming Units)
  - Temporal Window (Time Based):
    - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions
    - Types :
        - Tumbling : Aggregate all events for that same time interval. 
			-- The number of cars within a three-minute window going through a toll station that has three tollbooths":
		        SELECT COUNT(*) AS Count, TollBoothId into stream_analytics 
			INTO [output] 
		        FROM iot_event_hub 
			TIMESTAMP By TS_Kafka  -- 
		        PARTITION BY PartitionId  --  (similar to window partition)
		        GROUP BY  tumbling_window(second,2), TollBoothId, PartitionId
		        ``
        - Hopping : Tumbling + Overlapping for standard time with previous Window
		GROUP BY HoppingWindow(second, 60, 30)
        - Sliding : Window is created by an event and window exists for set time 
		GROUP BY slidingwindow(minute,1)
        - Session : 
		Window Cannot Overlap 
		Stops when stream stops and Resume if stream resumes 
		Window can have 1 or more event bit not reverse 
		Avoid Nulls by capturing Empty Windw 
		GROUP BY SessionWindow(second, 20, 60) 
        - Snapshot : Select count(*) into T from T1 group by system.timestamp()
		GROUP BY kafka_TS

  - Streaming : When the Backlogged Input Events metric > zero, the job is not able to process all incoming events.Increase SU (Streaming Units) 
-------------------
- Microsoft Purview (LeanIX ) 

	- Catalog and Governance and Audit
	- data lineage (master key in the Azure SQL database for lineage to work.)
	- Governance (Microsoft Purview governance portal)
	- Avoid Data Duplication 
	- Consists of Data Owner (Policy Creators), Stewards(enforce policy) , publishers , Consumers(Data Engineers)  
	- Store Information on type of Data which exists , Record Chnages of Data LandScape in a Company
	- Can have info on Storage Sources , Resources ,Data etc., 
	- Register Data
	- Pre-Req: Managed Identity
	- Open Purview GUI
	- Data Map Tap> Register button> Select Source Type > Give Name and fill details (Make sure the resource is linked to Managed Identity)
	- Open the Saved Map > click Scan
	- Synapse has inbuilt Purview Interface : Manage > Microsoft Purview . ( Everything done with managed identity is trackedfor synapse)
	
	- Create PurView > Add DataSource > Scan using Managed Identity > Analyse Result using PBI or Builtin
	- master key in the Azure SQL database for lineage to work.
	- Microsoft Purview governance portal : view and search Metadata
------------
- Lambda vs kappa
  - Lambda : Raw and Streaming served by different activities in same pipeline
  - Kappa : Both Raw and Streaming is done by same activity in same pipleien (Trigger =once , continues /processingtime))
-------------------
- Azure HDInsight
  - Apache Spark
  - Apache Hive
  - LLAP
  - Apache Kafka
  - Hadoop and more, in your Azure environment.
-------------------
- Databricks 
  - Simialr to Synapse Notebooks 
  - DBFS (Databricks File System) ,Similar to Synape SQL Database in Gen2 . User can query this like in Synapse
  - File extension .dfs
  - Uses Delta Lake Architecture
  - Spark Clusters
  - Main Difference 
	-  Only Apache Spark
	-  Magic Commands single "%"
        -  No Dataflows
  - SELECT /*+ SKEW('orders', 'o_custId') */ * FROM orders, customers WHERE o_custId = c_custId; --Skew Data
  -  Types:
    - Standard Cluster: General-purpose, suitable for various workloads.
    - High Concurrency Cluster: Optimized for concurrent users and low-latency queries.
    - Job Cluster: Designed for scheduled jobs, cost-effective with auto-termination.
    - Interactive Cluster: For ad-hoc analysis and interactive workloads.
    - Single Node Cluster: For development and testing with a single node.
    - Autoscaling Cluster: Automatically adjusts resources based on demand.
-------------------
- Difference Bwtween : https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing
  - Azure Stream Analytics
  - HDInsight with Spark Streaming
  - Apache Spark in Azure Databricks
  - Apache Kafka streams API
  - Azure Functions
  - Azure App Service WebJobs
-------------------
- ADF:
  - https://learn.microsoft.com/en-us/azure/data-factory/monitor-configure-diagnostics  
  - CLI cmd to tart ADF  $Invoke-AzDataFactoryV2Pipeline
  - https://learn.microsoft.com/en-us/azure/data-factory/pipeline-trigger-troubleshoot-guide
  -  Settings > Diagnostci Setting > Send to Log Analytics>Various logs relevant to your workloads to send to Log Analytics tables. 
-------------------
- Hive :
  - https://learn.microsoft.com/en-us/azure/data-factory/tutorial-transform-data-hive-virtual-network
  - HDInsight cluster URL :  https://<clustername>.azurehdinsight.net.
  - CREATE EXTERNAL TABLE T (id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' 
    STORED AS TEXTFILE LOCATION 'wasb://data@datastg.blob.core.windows.net/devices/';
    INSERT OVERWRITE TABLE T
    Select id FROM T2
-------------------

## Glossary
### General
- Data integration:--	  Establishing links between data
- Data consolidation:--	To extract Data from multiple data sources into a consistent structure
- Data Lake Storage Security:--   Access control lists (ACLs) and Portable Operating System Interface (POSIX) permissions -you can set permissions at a directory level or file level
- Data Lake redundancy:--	provide data redundancy in a single data center with locally redundant storage (LRS), or to a secondary region by using the Geo-redundant storage (GRS) option.
- Data Lake Hierarchical Namespace:--	To enable  you can select the option to Enable hierarchical namespace while creating Storage account
- Blob vs GEn2:--	blob = flat namespace , Gen2 = directories
- Synapse:--	https:--//microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/01-Explore-Azure-Synapse.html
- Cosmos DB and Azure SQL:--	Can be used an Operational / Transaction DB as they support data Auto sync with Synapse (analytical store support)
- **Database template**:	Templetes offered in Synapse to create out of the box databasese like - **Agriculture , Automotive etc.,

### Security
- Microsoft Entra ID:--	"directory service contain secured objects - user accounts , Resource Accounts 
 If a user and an instance of Azure Synapse Analytics are part of the same Microsoft Entra ID ,then user can access synapse"
- Azure Active Directory (Entra ID):--	Used to Create 1 Functional ID (secret ) called Serive Principle and used across all applications to communicate with each other
- Managed identity (service principal):--	managed by Azure
- SP (Serivce Principle):--	Managed by User ie., renew etc.,
- FireWalls:--	IP address that is allowed or denied access to an Azure Synapse workspace
- Azure Virtual Network (VNet):--	Azure resources to securely communicate with other virtual networks,
- Managed workspace Virtual Network:--	The Virtual Network managed by Azure Synapse
- Private endpoints:--	"enables you to securely connect up its various components through endpoints
 uses a private IP address from your Virtual Network"
- Managed private endpoints:--	Private endpoints in Managed workspace Virtual Network.
- Conditional Access:--	if a user wants to access a resource, then they must complete Like Authentication
- SQL Authentication:--	Usn /Pass
- SAS:--	A security token that can be attached to a URI (bearer)
- Transparent data encryption:-- Encrypting data at rest (Dedicated and Blob) ie., Complete Container is encrypted

-----------------------
## Links
- storage accounts \ PolyBase
  - https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview
  - https://learn.microsoft.com/en-us/training/modules/secure-azure-storage-account
  - https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control
- Synapse :
  - https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/2-understand-network-security-options
- Stream Analytics (OnDemand - video 9)
	  - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-solution-patterns
	  - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization
	  - https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions
- Purview
	- https://learn.microsoft.com/en-us/purview/concept-data-lineage
	- https://learn.microsoft.com/en-us/purview/unified-catalog-search
- storage accounts \ PolyBase
	  - https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview
	  - https://learn.microsoft.com/en-us/training/modules/secure-azure-storage-account
	  - https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control
- Dedicated SQL
	  - Change data capture VS change tracking Vs merge replication Vs snapshot replication
	    - Change Tracking: 
	      - Uses a internal log 
	      - lightweight 
	      -  Tracks row was changed without tracking the data that was changed
	
	    - Change Data Capture (CDC):
	        - Internally uses a seperate Table (sys.sp_cdc_enable_table)
	        - Identifies Each Row That Has Changed
	        - Better than Change tracking
	
	    - Merge Replication: 
	      - complex solution 
	      - Merge
	
	    - Snapshot Replication: 
	      - snapshot of the data at a specific point in time 
	      -  does not track changes incrementally.
	  - https://learn.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning
	  - https://learn.microsoft.com/en-us/training/modules/choose-storage-approach-in-azure/
	  - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/massively-parallel-processing-mpp-architecture 
	  - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/quickstart-scale-compute-tsql#check-dedicated-sql-pool-formerly-sql-dw-state
	  - https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/
	  - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-waiting-queries
